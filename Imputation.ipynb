{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             \n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523021, 36)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_imputed=train_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the variables that contain NANs (will need to be imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable that will need to be imputed:\n",
      " ['CloudCover' 'Events' 'Max_Gust_SpeedKm_h' 'Max_VisibilityKm'\n",
      " 'Mean_VisibilityKm' 'Min_VisibilitykM']\n"
     ]
    }
   ],
   "source": [
    "#Isnull() checks for nans\n",
    "#any() returns true if there is any true in an array\n",
    "#values() then converts everything into a single array\n",
    "areasWithNansMask = train_data_imputed.isnull().any().values\n",
    "\n",
    "#This is how we access column titles\n",
    "areasWithNans = train_data_imputed.columns.values\n",
    "\n",
    "#using boolean indexing here\n",
    "areasWithNans = areasWithNans[areasWithNansMask]\n",
    "print(\"Variable that will need to be imputed:\\n\", areasWithNans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the nan and non nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable that will need to be imputed ['CloudCover' 'Events' 'Max_Gust_SpeedKm_h' 'Max_VisibilityKm'\n",
      " 'Mean_VisibilityKm' 'Min_VisibilitykM']\n"
     ]
    }
   ],
   "source": [
    "#Isnull() checks for nans\n",
    "#any() returns true if there is any true in an array\n",
    "#values() then converts everything into a single array\n",
    "areasWithNansMask = train_data.isnull().any().values\n",
    "\n",
    "#This is how we access column titles\n",
    "areasWithNans = train_data.columns.values\n",
    "\n",
    "#using boolean indexing here\n",
    "areasWithNans = areasWithNans[areasWithNansMask]\n",
    "print(\"Variable that will need to be imputed\", areasWithNans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variables and the count of null and not null \n",
      "\n",
      "CloudCover Null values:\n",
      " Not Null    481840\n",
      "Null         41181\n",
      "Name: CloudCover, dtype: int64\n",
      "Null values percentage:  7.87 % \n",
      "\n",
      "Events Null values:\n",
      " Not Null    398923\n",
      "Null        124098\n",
      "Name: Events, dtype: int64\n",
      "Null values percentage:  23.73 % \n",
      "\n",
      "Max_Gust_SpeedKm_h Null values:\n",
      " Null        409947\n",
      "Not Null    113074\n",
      "Name: Max_Gust_SpeedKm_h, dtype: int64\n",
      "Null values percentage:  78.38 % \n",
      "\n",
      "Max_VisibilityKm Null values:\n",
      " Not Null    511683\n",
      "Null         11338\n",
      "Name: Max_VisibilityKm, dtype: int64\n",
      "Null values percentage:  2.17 % \n",
      "\n",
      "Mean_VisibilityKm Null values:\n",
      " Not Null    511683\n",
      "Null         11338\n",
      "Name: Mean_VisibilityKm, dtype: int64\n",
      "Null values percentage:  2.17 % \n",
      "\n",
      "Min_VisibilitykM Null values:\n",
      " Not Null    511683\n",
      "Null         11338\n",
      "Name: Min_VisibilitykM, dtype: int64 \n",
      "\n",
      "Null values percentage:  2.17 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples=len(train_data)\n",
    "print(\"The variables and the count of null and not null \\n\")\n",
    "#After using the isnull function, replace all instances of false to string \"not null\" and true instances to \"Null\"\n",
    "#This makes it much more readable when using the value_counts function\n",
    "cloudCoverPrintList = train_data['CloudCover'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"CloudCover Null values:\\n\",cloudCoverPrintList)\n",
    "nCloudCover=len(train_data[train_data[\"CloudCover\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nCloudCover/samples*100),\"%\", \"\\n\")\n",
    "\n",
    "eventsPrintList = train_data['Events'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"Events Null values:\\n\",eventsPrintList)\n",
    "nEvents=len(train_data[train_data[\"Events\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nEvents/samples*100),\"%\", \"\\n\")\n",
    "\n",
    "\n",
    "maxGustPrintList = train_data['Max_Gust_SpeedKm_h'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"Max_Gust_SpeedKm_h Null values:\\n\",maxGustPrintList)\n",
    "nMax_Gust_SpeedKm_h=len(train_data[train_data[\"Max_Gust_SpeedKm_h\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nMax_Gust_SpeedKm_h/samples*100),\"%\", \"\\n\")\n",
    "\n",
    "\n",
    "maxVisPrintList=train_data['Max_VisibilityKm'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"Max_VisibilityKm Null values:\\n\",maxVisPrintList)\n",
    "nMax_VisibilityKm=len(train_data[train_data[\"Max_VisibilityKm\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nMax_VisibilityKm/samples*100),\"%\", \"\\n\")\n",
    "\n",
    "\n",
    "meanVisPrintList=train_data['Mean_VisibilityKm'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"Mean_VisibilityKm Null values:\\n\", meanVisPrintList)\n",
    "nMean_VisibilityKm=len(train_data[train_data[\"Mean_VisibilityKm\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nMean_VisibilityKm/samples*100),\"%\", \"\\n\")\n",
    "\n",
    "\n",
    "minVisPrintList=train_data['Min_VisibilitykM'].isnull().replace(False, \"Not Null\").replace(True, \"Null\").value_counts()\n",
    "print(\"Min_VisibilitykM Null values:\\n\",minVisPrintList , \"\\n\")\n",
    "nminVisPrintList=len(train_data[train_data[\"Min_VisibilitykM\"].isnull()])\n",
    "print(\"Null values percentage:  %.2f\" % (nminVisPrintList/samples*100),\"%\", \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Max_Gust_SpeedKm_h*** column\n",
    "### Imputation dropping rows\n",
    "We start with dropping the Max_Gust_SpeedKm_h column since it has too many missing values >78%  (as seen above in the variable count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datase without Max gust speed now: \n",
      "\n",
      "(523021, 35)\n"
     ]
    }
   ],
   "source": [
    "#Drop Max_Gust regardless because it has too many missing values\n",
    "train_data_imputed = train_data_imputed.drop([\"Max_Gust_SpeedKm_h\"], axis=1)\n",
    "#print(\"Without Max gust speed now: \\n\",new.head())\n",
    "print(\"Datase without Max gust speed now: \\n\")\n",
    "print(train_data_imputed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Max_Gust_SpeedKm_h*** column\n",
    "### Imputation splitting events and adding *Clear* when NAN\n",
    "Here we fix the events column, \n",
    "converting nans to some other value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Nan Values with clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droping rows with nans after fixing the events column\n",
      "Without Events now: \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "print(\"droping rows with nans after fixing the events column\")\n",
    "train_data_imputed['Events'] = train_data_imputed['Events'].fillna(\"Clear\")\n",
    "print(\"Without Events now: \\n\", train_data_imputed[\"Events\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform values into dummies - copy pasted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes before making the dummies: 35\n",
      "Number of attributes after making the dummies: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes before making the dummies: \"+str(train_data_imputed.shape[1]))\n",
    "dummies_event=train_data_imputed['Events'].str.get_dummies(sep='-')\n",
    "train_data_imputed=train_data_imputed.drop('Events',axis=1)\n",
    "train_data_imputed=pd.concat([train_data_imputed,dummies_event],axis=1)\n",
    "print(\"Number of attributes after making the dummies: \"+str(train_data_imputed.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***CloudCover*** colum\n",
    "### Nan values to 0.0 \n",
    "Set *CloudCover* to 0 if NaN,considering that everytime CloudCover is NaN (but 56 times), Precipitationmm is always 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imputed['CloudCover'] = train_data_imputed['CloudCover'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***VisibilitykM*** colum\n",
    "### Drop nan values\n",
    "I was wondering if it is possible to impute with the mean of that store\n",
    "Or even if the day before or after make sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imputed =train_data_imputed.dropna(subset=['Max_VisibilityKm','Min_VisibilitykM','Mean_VisibilityKm']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StoreID                       0\n",
       "Date                          0\n",
       "IsHoliday                     0\n",
       "IsOpen                        0\n",
       "HasPromotions                 0\n",
       "StoreType                     0\n",
       "AssortmentType                0\n",
       "NearestCompetitor             0\n",
       "Region                        0\n",
       "NumberOfCustomers             0\n",
       "NumberOfSales                 0\n",
       "Region_AreaKM2                0\n",
       "Region_GDP                    0\n",
       "Region_PopulationK            0\n",
       "CloudCover                    0\n",
       "Max_Dew_PointC                0\n",
       "Max_Humidity                  0\n",
       "Max_Sea_Level_PressurehPa     0\n",
       "Max_TemperatureC              0\n",
       "Max_VisibilityKm              0\n",
       "Max_Wind_SpeedKm_h            0\n",
       "Mean_Dew_PointC               0\n",
       "Mean_Humidity                 0\n",
       "Mean_Sea_Level_PressurehPa    0\n",
       "Mean_TemperatureC             0\n",
       "Mean_VisibilityKm             0\n",
       "Mean_Wind_SpeedKm_h           0\n",
       "Min_Dew_PointC                0\n",
       "Min_Humidity                  0\n",
       "Min_Sea_Level_PressurehPa     0\n",
       "Min_TemperatureC              0\n",
       "Min_VisibilitykM              0\n",
       "Precipitationmm               0\n",
       "WindDirDegrees                0\n",
       "Clear                         0\n",
       "Fog                           0\n",
       "Hail                          0\n",
       "Rain                          0\n",
       "Snow                          0\n",
       "Thunderstorm                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_per_columns=train_data_imputed.isnull().sum()\n",
    "unknown_per_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform categorical Values into dummies \n",
    "### ***StoreType*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_event=train_data_imputed['StoreType'].str.get_dummies()\n",
    "train_data_imputed=train_data_imputed.drop('StoreType',axis=1)\n",
    "train_data_imputed=pd.concat([train_data_imputed,dummies_event],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Assortment*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_event=train_data_imputed['AssortmentType'].str.get_dummies()\n",
    "train_data_imputed=train_data_imputed.drop('AssortmentType',axis=1)\n",
    "train_data_imputed=pd.concat([train_data_imputed,dummies_event],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imputed['Date']=pd.to_datetime(train_data_imputed['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imputed.columns=train_data_imputed.columns.str.replace('\\s+', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save imputed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imputed.to_csv('./data/dataset_imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
